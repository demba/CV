%
% Demba Ba - NSF Biosketch
%
\pdfminorversion=4
\documentclass[12pt]{article}

\makeatletter
\let\saved@bibitem\@bibitem % < -- save to prevent problems due to the command getting redefined...
\makeatother

\usepackage{etaremune}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{bibentry}
\PassOptionsToPackage{hyphens}{url}\usepackage[dvips]{hyperref}
\usepackage{geometry}
\usepackage[yyyymmdd,hhmmss]{datetime}
\urlstyle{same}

\usepackage{amsmath}

% Fonts
\ifdefined\ispdf
    \usepackage[urw-garamond]{mathdesign}
    \usepackage[T1]{fontenc}
\else
    \usepackage[T1]{fontenc}
\fi

\def\HCode#1{}

% Set your name here
\def\name{Demba Ba}

% The following metadata will show up in the PDF properties
\hypersetup{
  colorlinks = true,
  urlcolor = gray,
  pdfauthor = {\name},
  pdfkeywords = {signal processing, computational neuroscience, scientific computing},
  pdftitle = {\name: Deep Sparse Signal Representations: Theory, Algorithms, and Applications to Multiscale Fusion},
  pdfsubject = {Deep Sparse Signal Representations},
  pdfpagemode = UseNone
}


\geometry{
  body={6.5in, 9.0in},
  left=1.0in,
  right=1.0in,
  top=1.0in
}

% Customize page headers
%%\pagestyle{myheadings}
%\markright{\name, Harvard University}
%\thispagestyle{empty}

% Custom section fonts
\usepackage{sectsty}
\sectionfont{\rmfamily\mdseries\Large}
\subsectionfont{\rmfamily\mdseries\itshape\large}

% Other possible font commands include:
% \ttfamily for teletype,
% \sffamily for sans serif,
% \bfseries for bold,
% \scshape for small caps,
% \normalsize, \large, \Large, \LARGE sizes.

% Don't indent paragraphs.
\setlength\parindent{0em}

% Make lists without bullets and compact spacing
\renewenvironment{itemize}{
  \begin{list}{}{
    %\setlength{\leftmargin}{1.5em}
    \setlength{\itemsep}{0.25em}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0.25em}
  }
}{
  \end{list}
}

%To highlight my own name in the bibliography
\def\FormatName#1{%
    \def\myname{Demba Ba}%
    \edef\name{#1}%
    \ifx\name\myname
      \textbf{#1}%
    \else
      #1%
    \fi
}

\begin{document}
\sloppy

\begingroup
\makeatletter
\let\@bibitem\saved@bibitem % <-- restore the original command immediately before use
\nobibliography{include/all}
\endgroup

% Place name at left
%\HCode{<div style="margin: 0 auto; overflow: hidden; width: 960px">} %row
\HCode{<div class="fluid-container"}

%\HCode{<div style="display: inline; float: left; margin: 0 10px; overflow: hidden; width: 820px">} %column
\HCode{<div class="row">}
\HCode{<div class="col-md-12">}
\HCode{<h1>}
{\huge \name}
\HCode{</h1>}
\HCode{</div>} %column
\HCode{</div>} %row

\bigskip

\HCode{<div class="row">}
\HCode{<div class="col-md-4">}
\begin{minipage}[t]{0.5\textwidth}
  Assistant Professor of Electrical Engineering \\
  and Bioengineering \\
  Harvard University \\
  School of Engineering and Applied Sciences \\
  Maxwell Dworkin \\
  33 Oxford st
  Cambridge, MA 02138 \\
\end{minipage}
\HCode{</div>} %end column
\HCode{<div class="col-md-8">}
\begin{minipage}[t]{0.5\textwidth}
  Phone: (617) 496-1228 \\
  %Fax: (301) 332-5264 \\
  %Office: MD 143 \\
  Email: \href{mailto:demba@seas.harvard.edu}{demba@seas.harvard.edu} \\
  Homepage: \href{http://demba-ba.org/}{http://demba-ba.org/} \\
  Group page: \href{http://crisp.seas.harvard.edu}{http://crisp.seas.harvard.edu/}
\end{minipage}
\HCode{</div>} %end column
\HCode{</div>} %end row

\HCode{<div class="row">}
\HCode{<div class="col-md-12">}
%\section*{Professional Preparation}
%
%%\begin{itemize}
%%    \item Ph.D./M.S. EECS, Massachusetts Institute of Technology, 2011/2016.
%%    %\item M.S. EECS, Massachusetts Institute of Technology, 2006.
%%    \item B.S. Electrical Engineering, University of Maryland, College Park, 2004.
%%\end{itemize}
%
%%\hspace{-0.075in}
%\begin{tabular}{llll}
%University of Maryland & College Park, MD & Electrical Engineering & B.S., 2004 \\
%Massachusetts Institute of Technology & Cambridge, MA & Electrical Engineering & M.S., 2006 \\
% &  & and Computer Science & \\
%Massachusetts Institute of Technology & Cambridge, MA & Electrical Engineering & Ph.D., 2011 \\
% &  & and Computer Science & \\
%Massachusetts Institute of Technology & Cambridge, MA & Computational & 2011--2015 \\
% &  & and Neuroscience & \\
%
%\end{tabular}

\section*{Research Statement}

The brain is a high-dimensional dynamical system that exhibits intricate dynamics at multiple spatial and temporal scales. As such, sophisticated data acquisition modalities-such as EEG, MEG, fMRI, calcium imaging and electrophysiology, to name a few-and behavioral experimental paradigms have been developed over the years to probe the dynamics of the brain at heterogeneous spatial and temporal scales. Two examples of the multiscale nature of neural activity come from electrophysiology and behavioral experiments. In electrophysiology, the spiking dynamics of neurons can exhibit variability both within a given trial (fast time scale) and across trials (fast time scale). In behavioral experiments, learning can involve dynamics within the confine of a session (fast time scale) and across sessions (slow time scale). State-of-the-art methods for analyzing neural data neglect their inherent multiscale nature.  My group develops efficient algorithms for estimation, inference and fusion in multiscale stochastic process models, with applications primarily (but not limited) to neuroscience. This work is at the interface of algorithmic computer science (e.g. trees, data-structures), dynamical modeling (e.g. state-space models), signal processing (e.g. filtering banks) and high-dimensional statistical inference and optimization (e.g. sparsity). In collaboration with experimentalists at MIT, Harvard Medical School and the University of Arizona, we have applied employed our tools to elucidate the role of dynamic networks of neurons in phenomena such as anesthesia, sleep, the learning of fear, and aging. The overarching goal of my research is to develop multiscale control algorithms that are able to fuse neural activity from multiple temporal and spatial scales (e.g. neural spiking activity, LFPs and EEG) to improve the state-of-the art of brain-machine interfaces.



\section*{Deep Sparse Signal Representations: Theory, Algorithms and Applications to Multiscale Fusion}

\subsection*{Motivation}

Stochastic processes that exhibit dynamics at multiple temporal or spatial
scales are pervasive in engineering and science. In health monitoring and neuroscience, there has been an increasing interest in taking a systems engineering approach to understanding and controlling the dynamics of the human body and the brain~\cite{kim2011epidermal2,liberman2013closed}. Indeed, over the past decade, human interaction with a variety of sensors that monitor and/or alter the activity of the autonomic (e.g. wearable bands~\cite{savage2015mobile}) and central nervous system~\cite{sejnowski2014putting} has grown exponentially. One salient feature of these sensor data is their heterogeneity, i.e. the presence of different temporal and spatial scales, as well as categories. At present, there lack principled approaches that
can fuse heterogeneous sources of data acquired at multiple temporal and spatial scales to identify
common underlying latent mulsticale states. More specifically, there is a need for a framework that can learn low-dimensional (e.g. sparse) latent representations of a system observed through high-dimensional multiscale observations, to obtain a provably adequate characterization of said system (e.g. heart or brain region/system), and then be used to design algorithms to control desired states of the system.I propose to develop a theoretical and algorithmic framework for estimation, inference and model comparison in \emph{Deep Sparse Representations of Signals}, with applications to neuroscience. 

\subsection*{Deep Sparse Signal Representations}

\emph{Deep Sparse Representations of Signals} are a generative model for multiscale stochastic processes that combine filter-bank theory~\cite{fliege1994multirate,strang1996wavelets,daubechies1992ten}, sparse recovery and compressed sensing~\cite{donoho2006compressed,candes2008introduction}, and state-space models~\cite{Ba:12,ba2013b} to arrive at a signal processing perspective and interpretation of deep neural networks. As detailed below, this new perspective provides several theoretical and algorithmic advantages over the classical approaches to deep neural networks~\cite{lecun2015deep}, and provides a natural solution to the problem of fusing data from multiple spatial and temporal scales, an important problem in computational neuroscience and numerous other scientific fields.

\subsection*{Deep Sparse Filter Banks: a principled alternative to deep neural nets}

Deep architectures~\cite{lecun2015deep} for neural networks and the accompanying learning algorithms have seen a resurgence over the past few years. One of the key appeals of deep learning has been its ability to yield predictive (e.g. class membership, cat or dog) hierarchical representations of nonlinear phenomena without the need for sophisticated a priori feature selection. The recent advances and success in a number of AI-related applications~\cite{bengio2009learning} are primarily a result of advances in computing and our ability to process massive amounts of data. One limitation of deep learning is the need for massive amounts of labelled data for learning. At present, there is significant interest in probabilistic generative models of deep networks~\cite{patel2015probabilistic} that do not require labelled data. %From a Bayesian perspective, generative models are appealing in small-data contexts as they allow one to balance prior and likelihood. From a Bayesian generative perspective, one reason why deep learning works is that access to massive amounts of data results in concentration of the posterior distribution  (data appears in likelihood term), so that the prior becomes less important. 
In classical signal processing and harmonic analysis, \emph{linear} multirate systems~\cite{fliege1994multirate} and filter banks~\cite{strang1996wavelets,daubechies1992ten} are the canonical framework for obtaining hierarchical representations of signals (analysis), and for generating signals from these representations (synthesis). I am developing a framework for studying deep generative hierarchical representations of signals based on \emph{nonlinear} filter banks. In linear filter banks, the filters in a given bank are specified as part of the synthesis step, allowing the \emph{generation} of signals. Specifying an architecture for a neural network is in fact akin to specifying an analysis operator. The filters in the the filter bank (synthesis) are related to the weights in a neural network, although there is not a one-to-one mapping between the two. Given the filters, the coefficients from the analysis step of the filter correspond to the hidden nodes in the network. Sparsity, and its interpretation as soft-thresholding in certain settings, provides the crucial nonlinearity that is one of the key components behind the success of deep architectures. The interpretation proposed here of deep architectures as nonlinear filter banks presents several advantages over current approaches

\begin{enumerate}
	\item \underline{Alleviating computational burden in ``big"-data problems}: One current challenge in the training of deep networks is the need to store the data to be used for training on chip/on GPU. Even when stochastic gradient descent (SGD) is used for training, this limits the applicability of deep learning approaches to high-resolution video and imaging, medical applications such as 4D ultrasound (3D volumes and time). Relating deep architectures to compressed sensing and the theory of sparse recovery and high-dimensional statistics opens door for ideas such as being able to estimate a deep network using random projections of the training data. This can lead to massive, theoretically-quantifiable, savings in terms of storage requirements, as well as practical algorithms.

	\item \underline{Learning deep filter banks from ``small" data}:  From a Bayesian generative model perspective, one key reason behind the success of deep learning is that access to massive amounts of data results in concentration (in a probabilistic sense) of the posterior distribution  (data appears in likelihood term), so that the prior becomes less important. Generative models are all the more appealing in ``small"-data contexts as they allow one to balance the prior and likelihood. In experimental neuroscience, where the cost of training and acquiring data from animals can become prohibitive over time, being able to train deep networks is a challenge at the moment because of the limited amount of data that are available. The framework proposed here based on generative model and sparsity lends itself naturally to Bayesian inference and estimation, which allows the incorporation and quantification of the uncertainty in network-weight estimates that result from ``small" data.
	
	\item \underline{Multsicale data fusion}: The framework proposed here, based on hierarchical representations of signal using nonlinear filter banks, provides a natural solution to the problem of fusing data from multiple temporal and/or spatial scales. Indeed, each level in the hierarchy corresponds to a different temporal/spatial scale. Being a generative model, the fusion problem is ``pops'' out of the inference step, which leverages the graphical structure in the hierarchy for efficiency.
	
	\item \underline{A signal processing perspective}: Lastly, despite the rich theory of linear multirate systems and filter banks in signal processing and harmonic analysis, there lacks in the literature (beyond that of Mallat for convolutional networks~\cite{bruna2013invariant}) a signal processing perspective to deep networks. Such a perspective would be a tremendous addition to the literature.
	
\end{enumerate}

\section*{Current Funding}

\section*{Pending Funding}

%\input{sections/mos_related_pubs}


%\section*{Appointments}
%
%\begin{itemize}
%    \item Harvard University, \emph{Assistant Professor of Electrical and Bioengineering (July 2015--Present)}
%
%    \item MIT Neuroscience Statistics Research lab, \emph{RA/Post-doc fellow (Sep. 2007--Aug. 2014)}
%
%    \item Google -- \emph{Summer Intern (June 2010--September 2010)}
%
%    \item Microsoft Research -- \emph{Summer Intern (June 2006/2009--September 2006/2009)}
%\end{itemize}

%\section*{Awards \& Honors}
%
%\begin{itemize}
%	\item \textbf{2016 Fellow in Neuroscience of the Alfred P. Sloan Foundation}
%    \item Spotlight Presentation at Advances in Neural Information Processing Systems 25 (NIPS 2012) [< 5\% acceptance rate]
%    \item ICME 2010 Best Student Paper Award (for summer 2009 work at MS Research)
%    \item University of Maryland Engineering honors citation
%    \item \textbf{A} \textbf{S}cholars \textbf{P}rograms for \textbf{I}ndustry-oriented \textbf{R}esearch in \textbf{E}ngineering
%\end{itemize}
%
%\section*{Relevant Coursework}
%
%Discrete-time Signal Processing, Stochastic Processes
%Detection and Estimation, Statistical Learning and Estimation, High-dimensional Statistics,
%Dynamic systems and Control, Advanced Computational Photography, Principles
%of Digital Communication, Abstract Linear Algebra, Real Analysis, Functional
%Analysis.

%\section*{Professional Registration}

%\begin{itemize}
%    \item Professional Engineer, Texas, \#118233
%\end{itemize}

%\HCode{<a name="journalpapers"></a>}
%\input{sections/books}
%\input{sections/oral_presentations}
%\input{sections/conf_proceedings}
%\input{sections/other_pubs}
%\input{sections/tech_reports}
%\input{sections/grants}
%\input{sections/courses}
%\input{sections/students}
%\input{sections/service}
%\input{sections/other}
%\newpage

%\section*{Synergistic Activities}
%
%\begin{itemize}
%  \item[\textbullet] Faculty Advisor for Harvard chapter of National Society of Black Engineers.
%  \item[\textbullet] Led the development of an open-source platform--\href{https://github.com/harvard/cloudJHub}{cloudJHub}--for serving Jupyter notebooks on the cloud and teaching data-science in a non-intimidating, cost-effective way.
%  \item[\textbullet] Member of Scientific Committee of 2017 Scientific Python Conference.
%  \item[\textbullet] Reviewer for a variety of signal processing and computational neuroscience journals.
%  \item[\textbullet] Advised undergraduate senior theses and served as a concentration mentor for undergraduate students (including one African-American female, one Hispanic-American female and one African male), advised visiting students (including one female student from China and one female student from Isra\"{e}l/Germany), 2015--present.
%\end{itemize}

%\section*{Collaborators in the Past 48 Months}

%Todd Coleman (Univ. Cal. San Diego), Emery Brown (MIT/Harvard), Vahid Tarokh (Harvard), Anne C. Smith (Harvard/Univ. Arizona), Carol Barnes (Univ. Arizona), Sarah Burke (Univ. Florida), Kay Tye (MIT), Stephen Allsop (MIT/Harvard), Luca Citi (Univ. Essex, UK), Massimilio Ponti (), Peter Latham (), Riccardo Poli (Univ. Essex, UK), Maryam Shanechi (Univ. Southern Cal.), Iannis Dimeris, Bejan Pesaran (NYU), Anne Churchland (Cold Spring Harbor Nat. Lab), Behtash Babadi, Patrick Purdon, Sridevi Sarma, Emad Eskandar, Gabriella Czanner, Uri Eden, Simona Temereanca, Wendy Suzuki, Taposh Banerjee, Wei Wu, Hubert Lim

%Gabe Shcamberg, Noa Shinitski, Yingzhuo Zhang,

%Stephen	Allsop (Harvard)	, Behtash Babadi (University of Maryland, College Park), Taposh Banerjee (Harvard), Carol Barnes	(University of Arizona), Emery Brown	(MIT/Harvard), Sarah Burke (University of Florida)	, Anne Churchland (Cold Spring Harbor Lab), Luca	Citi (University of Essex), Todd Coleman (University of California, San Diego), Gabriella Czanner (University of Liverpool), Yiannis Demiris (Imperial College London), Uri	Eden (Boston University), Emad Eskandar (Harvard), Peter	Latham (University College London), Hubert Lim (University of Minnesota), Bijan Pesaran (NYU), Riccardo Poli	 (University of Essex), 	Massimiliano Ponti (University College London), Patrick Purdon (Harvard), Sridevi Sarma (John Hopkins University), Maryam Shanechi (USC), Anne	Smith (University of Arizona/Harvard), Wendy Suzuki (NYU), Vahid Tarokh (Harvard), Simona Temereanca (Harvard), Kay Tye (MIT), Wei Wu (Stanford University), Danilo Mandic (Imperical College London), Michael Jordan (University of California, Berkeley).

%\vspace{-0.15in}


%\section*{Graduate and Postdoctoral Advisors}
%
%\begin{itemize}
%	\item[] Graduate and postdoctoral advisor: Emery N. Brown (MIT/Harvard)
%\end{itemize}
%\vspace{-0.25in}
%\section*{Advisees}
%
%\begin{itemize}
%
%	\item \textbf{Postdoctoral researcher}: Taposh Banerjee.
%	\item \textbf{Ph.D. students}: Yingzhuo Zhang (Harvard, Female); Bahareh Tolooshams (Harvard, Female); Edward Nobikhov (Harvard), Andrew Song (MIT).
%	\item \textbf{Master students}: Xu Si (Harvard, Female); Courtney Cochrane (Harvard, Female), Jiejun Lu (Summer 2016 Visitor, Female, China); Noa Shinitski (Summer 2016 Visitor, Female, Isra\"{e}l/Germany).
%	\item \textbf{Harvard Undergraduates}: Ryan Halvorson; Jerry Chang; Ciara Adkins (AfricanAmerican Female); Alina Acosta (Hispanic-American Female).
%
%\end{itemize}


%\input{sections/references}

\bibliographystyle{unsrt}
\bibliography{include/all}

%\vfill
% Footer
%\HCode{<center>}
%\begin{center}
%    \begin{small}
%        Last updated: \today\ at \currenttime
%    \end{small}
%\end{center}
%\HCode{</center>}

%\bibliographystyle{myplainurl}
%\bibliography{all}

\HCode{</div>} %column
\HCode{</div>} %row
\HCode{</div>} %fluid-container

\end{document}
